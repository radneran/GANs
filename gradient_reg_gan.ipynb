{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from mog_eigval_dist import *\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-D dist\n",
    "# target dist -- normal ([-1, 1], 0.4)\n",
    "# aux dist -- normal ([1, -1], 0.4)\n",
    "# input dist -- normal (0, 1)\n",
    "tmu1 = [-1, 1]\n",
    "tmu2 = [1, -1]\n",
    "amu1 = [-1, -1]\n",
    "amu2 = [1, 1]\n",
    "sigma = [0.4, 0.4]\n",
    "batch_size = 512\n",
    "inputs = np.random.normal(loc=[0,0], scale=[0.4,0.4], size=[batch_size, 2])\n",
    "train1 = np.random.normal(loc=[-1,1], scale=[0.4,0.4], size=[batch_size, 2])\n",
    "train2 = np.random.normal(loc=[1,-1], scale=[0.4,0.4], size=[batch_size, 2])\n",
    "aux1 = np.random.normal(loc=[-1,-1], scale=[0.4,0.4], size=[batch_size, 2])\n",
    "aux2 = np.random.normal(loc=[1,1], scale=[0.4,0.4], size=[batch_size, 2])\n",
    "\n",
    "out = np.vstack([train1, train2, aux1, aux2])\n",
    "kde(out[:,0], out[:,1], save_file='tgtauxmog.png')\n",
    "kde(inputs[:,0], inputs[:,1])\n",
    "plt.plot(inputs[:,0], inputs[:,1], 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_load_model(net, model_name):\n",
    "    _,_,sd,_,_ = load_checkpoint(model_name)\n",
    "    net.load_state_dict(sd)\n",
    "    \n",
    "def sample_dist(tmu, tsig, batch_size=512, n=1):\n",
    "    t = np.random.normal(loc=tmu, scale=tsig, size=[batch_size//n, 2])\n",
    "    return t\n",
    "def get_label(n, l, size=1):\n",
    "    # return one-hot encoding of length l with nth element 1\n",
    "    # return torch.FloatTensor(np.identity(n=l)[n,:].reshape(1,2)).expand(size, -1).contiguous()\n",
    "    return np.repeat(np.identity(n=l)[n,:].reshape(1,2), size, axis=0)\n",
    "\n",
    "def train_classifier(net, opt, loss_fn, tmus, tsigs, model_name, iterations=4000, batch_size=512):\n",
    "    assert(len(tmus) == len(tsigs))\n",
    "    ndist = len(tmus)\n",
    "    for iteration in range(iterations):\n",
    "        opt.zero_grad()\n",
    "        inputs = torch.FloatTensor(np.vstack([sample_dist(tmus[i], tsigs[i], n=ndist) \n",
    "                                             for i in range(ndist)])).cuda()\n",
    "        labels = torch.LongTensor(np.hstack([[i]*(batch_size//ndist) for i in range(ndist)])).cuda()\n",
    "        loss = loss_fn(net(inputs), labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print('[%d/%d] Loss: %f'%(iteration, iterations, loss))\n",
    "        if iteration%999==0:\n",
    "            save_checkpoint(epoch=0, iters=iteration, net=net, optim=opt, model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_classifier = MLP_Discriminator(depth=4, width=16, activation=SELU,insize=2,outsize=2).cuda()\n",
    "for fc in get_modules_of_type(module_type=nn.Linear, net=tgt_classifier):\n",
    "    selu_init(fc)\n",
    "tc_optim = optim.Adam(tgt_classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier(tgt_classifier, tc_optim, nn.CrossEntropyLoss(), [tmu1, tmu2], [sigma, sigma], \"MLPclass_tgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mi model\n",
    "def train_mi(net, opt, tgt, tmus, amus, sigma=sigma, loss_fn=nn.BCELoss(), model_name='MLPmi', iterations=4000,batch_size=512):\n",
    "    assert(len(tmus) == len(amus) == len(sigma))\n",
    "    ndist = len(tmus)\n",
    "    for iteration in range(iterations):\n",
    "        opt.zero_grad()\n",
    "        traindist = torch.FloatTensor(np.vstack([sample_dist(mu, sigma, n=ndist) for mu in tmus])).cuda()\n",
    "        auxdist = torch.FloatTensor(np.vstack([sample_dist(mu, sigma, n=ndist) for mu in amus])).cuda()\n",
    "        #print(traindist.shape, auxdist.shape)\n",
    "        trainout = tgt(traindist)\n",
    "        auxout = tgt(auxdist)\n",
    "        out = torch.cat([trainout, auxout], dim=0)\n",
    "        #print(out.shape)\n",
    "        labels = torch.FloatTensor(np.vstack([[[np.abs(i-1)]]*batch_size for i in range(ndist)])).cuda()\n",
    "        #labels = torch.FloatTensor(np.vstack([get_label(i, ndist, batch_size) for i in range(ndist)])).cuda()\n",
    "        #print(labels.shape, labels)\n",
    "        #loss = loss_fn(net(out), labels)\n",
    "        loss = -(labels * torch.log(torch.sigmoid(net(out))) + (1 - labels) * torch.log(1 - torch.sigmoid(net(out)))).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print('[%d/%d] Loss: %f'%(iteration, iterations, loss))\n",
    "        if iteration%999==0:\n",
    "            save_checkpoint(epoch=0, iters=iteration, net=net, optim=opt, model_name=model_name)\n",
    "# MI adversarial training\n",
    "def atrain_mi(tgt, mic, m_opt, G, g_opt, rsampler, isampler=None, model_name='MIadv', iterations=8000):\n",
    "    for iteration in range(iterations):\n",
    "        niters = 100 if iteration < 25 or iteration % 500 == 0 else 5\n",
    "        for i in range(niters):\n",
    "            real = rsampler()\n",
    "            fake = G(isampler()) if isampler else G()\n",
    "            L_D = mic(tgt(real)).mean() - mic(tgt(fake.detach())).mean() + calc_gradient_penalty(mic, tgt(real), \n",
    "                                                                                                 tgt(fake.detach()))\n",
    "            m_opt.zero_grad()\n",
    "            L_D.backward()\n",
    "            m_opt.step()\n",
    "        g_opt.zero_grad()\n",
    "        L_G = mic(tgt(fake)).mean()\n",
    "        L_G.backward()\n",
    "        g_opt.step()\n",
    "        print('[%d/%d] Loss Mic: %f Loss G: %f'%(iteration, iterations, L_D, L_G))\n",
    "        if iteration % 1000 == 999:\n",
    "            save_checkpoint(epoch=0, iters=iteration, net=G, optim=g_opt, model_name=model_name+'_G')\n",
    "            save_checkpoint(epoch=0, iters=iteration, net=mic, optim=m_opt, model_name=model_name+'_Mic')\n",
    "            fake = fake.detach().cpu().numpy()\n",
    "            kde(fake[:,0],fake[:,1], show=False, save_file=\"./expt_results/{0}_{1}.png\".format(model_name, iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_classifier = MLP_Discriminator(depth=16, width=16, activation=SELU,insize=2,outsize=1).cuda()\n",
    "G = MLP_Generator(depth=10, width=16, activation=SELU,bs=512,insize=2,outsize=2).cuda()\n",
    "\n",
    "for fc in get_modules_of_type(module_type=nn.Linear, net=G):\n",
    "    selu_init(fc)\n",
    "g_optim = optim.Adam(G.parameters(), lr=1e-4)\n",
    "for fc in get_modules_of_type(mi_classifier, nn.Linear):\n",
    "    selu_init(fc)\n",
    "mi_opt = optim.Adam(mi_classifier.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_classifier = MLP_Discriminator(depth=4, width=16, activation=SELU,insize=2,outsize=2).cuda()\n",
    "_,_,tgt_sd,_,_ = load_checkpoint(model_name=\"MLPclass_tgt\")\n",
    "tgt_classifier.load_state_dict(tgt_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mi(mi_classifier, mi_opt, tgt_classifier, [tmu1, tmu2], [amu1, amu2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrain_mi(tgt_classifier, mi_classifier, mi_opt, G, g_optim, sampler([tmu1, tmu2], [sigma, sigma], 512), model_name='MIadv16', \n",
    "          iterations=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = MLP_Generator(depth=10, width=16, activation=SELU,bs=512,insize=2,outsize=2).cuda()\n",
    "D = MLP_Discriminator(depth=4, width=16, activation=SELU,insize=2,outsize=1).cuda()\n",
    "\n",
    "for fc in get_modules_of_type(module_type=nn.Linear, net=G):\n",
    "    selu_init(fc)\n",
    "for fc in get_modules_of_type(module_type=nn.Linear, net=D):\n",
    "    selu_init(fc)\n",
    "\n",
    "#g_optim = optim.RMSprop(G.parameters(), alpha=5e-4)\n",
    "#d_optim = optim.RMSprop(D.parameters(), alpha=5e-4)\n",
    "g_optim = optim.Adam(G.parameters(), lr=1e-4)\n",
    "d_optim = optim.Adam(D.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = G(torch.FloatTensor(inputs).cuda()).detach().cpu().numpy()\n",
    "kde(out[:,0], out[:,1], save_file='./expt_results/WMLPG_gp_init1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampler:\n",
    "    def __init__(self, mus, sigs, bs):\n",
    "        assert len(mus) == len(sigs)\n",
    "        self.mus = mus\n",
    "        self.sigs = sigs\n",
    "        self.bs = bs\n",
    "        self.ndist = len(mus)\n",
    "    def __call__(self):\n",
    "        samples = np.vstack([sample_dist(self.mus[i], self.sigs[i],batch_size=self.bs, n=self.ndist) for i in range(self.ndist)])\n",
    "        return torch.FloatTensor(samples).cuda()\n",
    "    \n",
    "class MiganToy:\n",
    "    def __init__(self, tgt, mic, iloader, aloader, G=None, D=None, g_opt = None, d_opt = None, model_name=\"Migan_toy\"):\n",
    "        self.tgt = tgt\n",
    "        self.mic = mic\n",
    "        self.iloader = iloader\n",
    "        self.aloader = aloader\n",
    "        self.G = G if G else MLP_Generator(depth=4, width=16, activation=SELU,bs=512,insize=2,outsize=2).cuda()\n",
    "        self.D = D if D else MLP_Discriminator(depth=4, width=16, activation=SELU,insize=2,outsize=1).cuda()\n",
    "        if not G:\n",
    "            for fc in get_modules_of_type(self.G, nn.Linear):\n",
    "                selu_init(fc)\n",
    "        if not D:\n",
    "            for fc in get_modules_of_type(self.D, nn.Linear):\n",
    "                selu_init(fc)    \n",
    "        self.g_opt = g_opt if g_opt else optim.Adam(self.G.parameters())\n",
    "        self.d_opt = d_opt if d_opt else optim.Adam(self.D.parameters())\n",
    "         \n",
    "    def get_migan_loss(self):\n",
    "        real, fake = self.aloader(), self.G(self.iloader())\n",
    "        L_D = self.D(real).mean() - self.D(fake.detach()).mean() + calc_gradient_penalty(D, tgt, fake.detach())\n",
    "        \n",
    "        \n",
    "def calc_gradient_penalty(netD, real_data, fake_data, LAMBDA=10, use_cuda=True, BATCH_SIZE=512):\n",
    "    # print \"real_data: \", real_data.size(), fake_data.size()\n",
    "    alpha = torch.rand(BATCH_SIZE, 1)\n",
    "    alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement()/BATCH_SIZE)).contiguous().view(BATCH_SIZE, 2)\n",
    "    alpha = alpha.cuda() if use_cuda else alpha\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    if use_cuda:\n",
    "        interpolates = interpolates.cuda()\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() if use_cuda else torch.ones(\n",
    "                                  disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n",
    "\n",
    "def train_gan(G, D, g_optim, d_optim, sampler_fn, model_name, batch_size=512, iter_offset=0, iterations=100000, clip=0.1):\n",
    "    for iteration in range(iterations):\n",
    "        \"\"\"L_D = -(torch.log(torch.sigmoid(D(tgt))).mean()\n",
    "                + torch.log(1 - torch.sigmoid(D(fake.detach()))).mean())\n",
    "        L_G = (-torch.log(torch.sigmoid(D(fake))).mean())\n",
    "        \"\"\"\n",
    "        niters = 100 if iteration < 25 or iteration % 500 == 0 else 5\n",
    "        #for p in G.parameters():\n",
    "        #    p.requires_grad = False\n",
    "        for i in range(niters):\n",
    "          #  for p in D.parameters():\n",
    "          #      p.data.clamp_(-clip, clip)\n",
    "            inputs = torch.FloatTensor(np.random.normal(loc=[0,0], scale=[0.4,0.4], size=[batch_size, 2])).cuda()\n",
    "            #tgt = torch.FloatTensor(np.random.normal(loc=tmu, scale=tsig, size=[batch_size, 2])).cuda()\n",
    "            tgt = sampler_fn()\n",
    "            fake = G(inputs)\n",
    "            L_D = D(tgt).mean() - D(fake.detach()).mean() + calc_gradient_penalty(D, tgt, fake.detach())\n",
    "            #L_D, L_G = loss_fn()\n",
    "            d_optim.zero_grad()\n",
    "            L_D.backward()\n",
    "            d_optim.step()\n",
    "        #for p in G.parameters():\n",
    "            #p.requires_grad = True\n",
    "        g_optim.zero_grad()\n",
    "        L_G = D(fake).mean()\n",
    "        L_G.backward()\n",
    "        g_optim.step()\n",
    "        \n",
    "        print('[%d/%d] Loss D: %f Loss G: %f'%(iteration, iterations, L_D, L_G))\n",
    "        if iteration % 1000 == 999:\n",
    "            save_checkpoint(epoch=0, iters=iteration, net=G, optim=g_optim, model_name=model_name+'_G')\n",
    "            save_checkpoint(epoch=0, iters=iteration, net=D, optim=d_optim, model_name=model_name+'_D')\n",
    "            fake = fake.detach().cpu().numpy()\n",
    "            kde(fake[:,0],fake[:,1], show=False, save_file=\"{0}_{1}.png\".format(model_name, iteration+iter_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(G, D, g_optim, d_optim, sampler([amu1, amu2], [[0.4,0.4]]*2,512), 'WMLP_toy_aux', iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = G(torch.FloatTensor(inputs).cuda()).detach().cpu().numpy()\n",
    "kde(out2[:,0], out2[:, 1])#save_file='./expt_results/WMLPG_gp_aux1.png')#bbox=[-10,10,-10,10] )#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = MLP_Discriminator(depth=4, width=32, activation=SELU,insize=2,outsize=1).cuda()\n",
    "print(mic)\n",
    "quick_load_model(mic, 'MIadv32_Mic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tgt_classifier(sampler([tmu1,tmu2],[sigma, sigma], 32)()))\n",
    "#print(tgt_classifier(sampler([[-40, 40],[40, -40]],[sigma, sigma], 32)()))\n",
    "print(mi_classifier(tgt_classifier(sampler([tmu1,tmu2],[sigma, sigma], 32)())))\n",
    "print(mi_classifier(tgt_classifier(sampler([[-40, 40],[40, -40]],[sigma, sigma], 32)())))\n",
    "print(mi_classifier(tgt_classifier(sampler([[-40, -40],[40, 40]],[sigma, sigma], 32)())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = MLP_Generator(depth=4, width=16, activation=SELU,bs=512,insize=2,outsize=2).cuda()\n",
    "for fc in get_modules_of_type(G, nn.Linear):\n",
    "    selu_init(fc)\n",
    "g_opt = optim.Adam(G.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 4000\n",
    "for iteration in range(iterations):\n",
    "    g_opt.zero_grad()\n",
    "    inputs = torch.FloatTensor(np.random.normal(loc=[0,0], scale=[0.4,0.4], size=[batch_size, 2])).cuda()\n",
    "    outputs = G(inputs)\n",
    "    loss = -torch.sigmoid(mi_classifier(tgt_classifier(outputs))).mean()\n",
    "    loss.backward()\n",
    "    g_opt.step()\n",
    "    print('[%d/%d] Loss: %f'%(iteration, iterations, loss))\n",
    "    if iteration%1000 == 999:\n",
    "        save_checkpoint(0, iteration, G, g_opt, model_name='migansimple')\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        kde(outputs[:,0], outputs[:,1], bbox=[np.amin(outputs,axis=0)[0], np.amax(outputs,axis=0)[0], \n",
    "                                              np.amin(outputs,axis=0)[1], np.amax(outputs,axis=0)[1]], \n",
    "            save_file=\"migansimple_{0}.png\".format(iteration))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.arange(2)*0\n",
    "for i in range(6):\n",
    "    i+=1\n",
    "    mat = np.vstack([mat, (np.arange(2)+i)*i])\n",
    "print(mat)\n",
    "print(mat[:,0])\n",
    "print(np.amax(mat[:,0], axis=0))\n",
    "print(np.amax(mat[:,0], axis=0))\n",
    "print(np.amin(mat[:,0], axis=0))\n",
    "print(np.amin(mat[:,0], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
